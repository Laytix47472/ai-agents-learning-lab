# ğŸ” Aprendizado Auto-supervisionado â€“ Contrastive Learning

Este paradigma nÃ£o exige rÃ³tulos humanos: os dados â€œensinam a si mesmosâ€ por meio de tarefas pretextuais. Ã‰ usado no prÃ©-treinamento de LLMs e embeddings de texto/imagem.

---

## ğŸ§  Exemplo: BERT Masked Language Modeling (MLM)

- Frase original: "A inteligÃªncia artificial estÃ¡ avanÃ§ando rÃ¡pido"
- Frase com mÃ¡scara: "A inteligÃªncia artificial estÃ¡ [MASK] rÃ¡pido"
- O modelo tenta prever "avanÃ§ando"

---

## ğŸ“Œ Outro exemplo: Contrastive Learning (Siamese Networks)

- Dados pareados positivos (ex: mesma pergunta com sinÃ´nimos)
- Dados pareados negativos (ex: perguntas sem relaÃ§Ã£o)

---

## ğŸ”§ Ferramentas

| Framework       | DescriÃ§Ã£o                            |
|------------------|--------------------------------------|
| HuggingFace Transformers | PrÃ©-treinamento de LLMs             |
| SimCSE            | Contrastive Learning para embeddings |
| DINOv2 (Meta AI)  | Aprendizado auto-supervisionado visual  |

---

## ğŸ§ª Exemplo com `transformers`

```python
from transformers import BertTokenizer, BertForMaskedLM
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForMaskedLM.from_pretrained("bert-base-uncased")

inputs = tokenizer("The sky is [MASK].", return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs).logits
```
---

## ğŸ“ AplicaÃ§Ã£o

Auto-supervisÃ£o Ã© base fundamental para agentes LLM, pois permite escalar treinamento sem custo de anotaÃ§Ã£o.

---
